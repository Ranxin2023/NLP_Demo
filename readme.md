# NLPDEMO: Natural Language Processing Hands-On Project
## Table of Contents
- [Introduction](#intruduction)
- [Project Structure](#project-structure)
- [Concepts](#nlp-concepts):
    - [What is NLP](#1-what-is-nlp)
    - [What are the main challenges of NLP](#2-what-are-the-main-challenges-in-nlp)
    - [Tokenization in NLP](#3-tokenization-in-nlp)
    - [Preprocessing](#4-what-are-some-common-preprocessing-techniques-used-in-nlp)
    - [Part of Speech](#5-part-of-speech-pos-tagging-in-nlp)
    - [Three Approaches](#7-what-are-the-differences-between-rule-based-statistical-based-and-neural-based-approaches-in-nlp)
    - [NLTK Package](#8-nltk-natural-language-toolkit)
    - [Bag of Words](#9-what-is-the-bag-of-words-bow-model)
    - [Parsing in NLP](#10-what-are-the-different-types-of-parsing-in-nlp)
    - [TF IDF](#11-what-is-tf-idf-in-nlp)
    - [Machine Learning Algorithms](#12-machine-learning-algorithms-used-in-nlp)
    - [Transformer](#13-transfomers)
        - [Interview Questions](#interview-questions)
    - [Word Embeddings](#14-word-embeddings)
    - [Position Encoding](#15-positional-encoding)
    - [BERT](#16-bert)
    - [Gensim](#17-gensim)
    - [OOV(out of words)](#18-oovout-of-vocabularywords)
    - [Machine Translation](#19-machine-translation)
    - [Sequence Labeling](#20-what-is-sequence-labeling)
    - [Layer Norm and Batch Norm](#21-layernorm-vs-batch-norm)
- [Python Dependencies](#python-dependencies)
- [Setup](#setup)

## Intruduction
This project is a hands-on demo of various Natural Language Processing (NLP) techniques, organized in modular Python scripts. It walks through fundamental concepts such as preprocessing, normalization, text classification, lemmatization, different NLP model approaches, and more.

Before going with tutorial, I suggest to study pytorch first: [Pytorch Tutorial](https://github.com/Ranxin2023/PytorchDemo). 

## Project Structure
```graphql
NLPDemo/
‚îÇ
‚îú‚îÄ‚îÄ datasets/                          # (Optional) Datasets used in demos (not shown yet)
‚îÇ
‚îú‚îÄ‚îÄ demo_code/
‚îÇ   ‚îú‚îÄ‚îÄ GeneralNLP/                    # General NLP tasks and preprocessing
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ nlp_challenges.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ nlp_tasks.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ preprocessing.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ three_approaches.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ ML_in_NLP/                     # ML models used in NLP tasks
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ transformer/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ datacamp_tutorial.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ self_implementation.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Transformer.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Decision_Tree.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ML_algorithms.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Naive_Bayes.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ SVM_demo.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ NLPUtils/                      # Utility demos and examples
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ bow_demo.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cos_similarity_examples.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ nltk_demo.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ parsing_demo.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ text_normalization.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tf_idf_demo.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Sequence_Labelling/           # Sequence labeling tasks
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chunking.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ NER_demo.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ semantic_role_labeling.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tagging_demo.py
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ Word_Embeddings/              # Word embedding algorithm demos
‚îÇ       ‚îú‚îÄ‚îÄ BERT.py
‚îÇ       ‚îú‚îÄ‚îÄ GloVe_demo.py
‚îÇ       ‚îú‚îÄ‚îÄ word2vec.py
‚îÇ       ‚îú‚îÄ‚îÄ word_embeddings.py
‚îÇ       ‚îî‚îÄ‚îÄ word_embedding_algorithms.py
‚îÇ
‚îú‚îÄ‚îÄ nlp_env/                          # Python virtual environment (ignored in Git)
‚îÇ
‚îú‚îÄ‚îÄ output_results/                   # Text outputs and logs from all demos
‚îÇ   ‚îú‚îÄ‚îÄ bow_demo.txt
‚îÇ   ‚îú‚îÄ‚îÄ cos_similarity_demo.txt
‚îÇ   ‚îú‚îÄ‚îÄ ML_demo.txt
‚îÇ   ‚îú‚îÄ‚îÄ nlp_challenges.txt
‚îÇ   ‚îú‚îÄ‚îÄ nlp_tasks.txt
‚îÇ   ‚îú‚îÄ‚îÄ nltk_demo.txt
‚îÇ   ‚îú‚îÄ‚îÄ parsing_result.txt
‚îÇ   ‚îú‚îÄ‚îÄ SRL_demo.txt
‚îÇ   ‚îú‚îÄ‚îÄ tag_demo.txt
‚îÇ   ‚îú‚îÄ‚îÄ text_normalization.txt
‚îÇ   ‚îú‚îÄ‚îÄ text_preprocessing.txt
‚îÇ   ‚îú‚îÄ‚îÄ tf_idf_result.txt
‚îÇ   ‚îú‚îÄ‚îÄ three_approach_result.txt
‚îÇ   ‚îú‚îÄ‚îÄ transformer_datacamp.txt
‚îÇ   ‚îú‚îÄ‚îÄ word_embeddings.txt
‚îÇ   ‚îî‚îÄ‚îÄ word_embeddings_algorithms.txt
‚îÇ
‚îú‚îÄ‚îÄ main.py                           # Entry point to run selected demos
‚îú‚îÄ‚îÄ readme.md                         # Project documentation
‚îú‚îÄ‚îÄ requirements.txt                  # Python dependencies
‚îî‚îÄ‚îÄ .gitignore                        # Files and folders ignored by Git


```

## NLP Concepts
### 1. What is NLP?
Natural Language Processing (NLP) is a subfield of Artificial Intelligence that focuses on the interaction between computers and human language. It enables machines to understand, interpret, and generate natural language in a way that is meaningful. NLP encompasses tasks such as translation, sentiment analysis, question answering, and more.

### 2. What are the main challenges in NLP?
The complexity and variety of human language create numerous difficult problems for the study of Natural Language Processing (NLP). The primary challenges in NLP are as follows:
#### Challenge 1: **Semantics and Meaning**:
- Example: "He saw a bat in the dark."
    - **Why it‚Äôs hard**: The word **"bat"** can mean an **animal** or a **sports object**. Determining which meaning is correct requires understanding the **context**.
    - **What we do**: We use spaCy to analyze each token‚Äôs **part of speech (POS)** and lemma to explore how it processes such ambiguity.

#### Challenge 2: Ambiguity (Lexical & Syntactic)
- Example: "She couldn't bear the pain."

    - **Why it‚Äôs hard**: The word "bear" could mean endure or refer to the animal. Without context, it's ambiguous.
    - **What we do**: Use TextBlob to extract sentiment, showing how ambiguity can influence interpretation.
    - **Real-world impact**: Ambiguity can mislead **emotion analysis**, **translation**, and **recommendation systems**.

#### Challenge 3: Contextual Understanding (Coreference Resolution)
- Example: "Alex told Jordan that he would win."

    - **Why it‚Äôs hard**: Who does **"he"** refer to‚ÄîAlex or Jordan? This is called a coreference problem.
    - **What we do**: Analyze dependency parsing with spaCy to show how it tracks grammatical relationships‚Äîbut note that spaCy doesn't resolve coreferences by default.
    - **Real-world impact**: Coreference errors can break summarization, QA systems, or AI assistants.
        - Coreference occurs when multiple expressions (words or phrases) in a text refer to the same real-world entity.
        - üîç Simple Example:
        ‚ÄúEmma went to the store. She bought apples.‚Äù
        - Here, ‚ÄúShe‚Äù refers to ‚ÄúEmma‚Äù. This is a coreference relationship.

#### Challenge 4: Language Diversity
- Example: `"‡§Æ‡•à‡§Ç ‡§ò‡§∞ ‡§ú‡§æ ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Å‡•§"` (Hindi for ‚ÄúI am going home.‚Äù)
    - **Why it‚Äôs hard**: Most NLP models are trained on English. When given non-English input, models often fail.
    - **What we do**: Feed a Hindi sentence to spaCy's English model and show it fails to assign meaningful POS tags.
    - **Real-world impact**: Lack of multilingual support limits accessibility of NLP tools for non-English speakers.

#### Challenge 5: Data Limitations and Bias
- Example: `"The nurse helped the doctor because she was tired."`
    - **Why it‚Äôs hard**: NLP systems often learn societal biases from training data (e.g., assuming "nurse" = female).
    - **What we do**: Show the dependency parse and discuss how gender pronouns often reinforce stereotypes in text understanding.
    - **Real-world impact**: Bias can lead to unethical outputs in tasks like resume screening, chatbots, and search.

### 3. Tokenization in NLP
#### 3.1   What is a Token?
A **token** is the smallest unit in a text that carries meaning. Depending on the level of tokenization, a token could be:
- A word (e.g., ‚Äútokenization‚Äù)

- A subword (e.g., ‚Äútoken‚Äù, ‚Äú##ization‚Äù in BERT)

- A character (e.g., ‚Äút‚Äù, ‚Äúo‚Äù, ‚Äúk‚Äù, ...)
#### 3.2 types of token
| Type                       | Description                                                              | Example                                     |
| -------------------------- | ------------------------------------------------------------------------ | ------------------------------------------- |
| **Word Tokenization**      | Splits text by words, considering punctuation and spacing                | `"NLP is great"` ‚Üí `["NLP", "is", "great"]` |
| **Subword Tokenization**   | Breaks rare or complex words into frequent sub-units (used in BERT, GPT) | `"tokenization"` ‚Üí `["token", "##ization"]` |
| **Character Tokenization** | Breaks every character                                                   | `"NLP"` ‚Üí `["N", "L", "P"]`                 |

### 4. What are some common preprocessing techniques used in NLP?
- Tokenization: Splitting text into words or phrases.

- Stop Word Removal: Removing common, less informative words.

- Text Normalization:

    - Lowercasing
    - Lemmatization
    - Stemming
    - Date/Time normalization

- Punctuation/Special Character Removal

- HTML Tag Removal

- Spell Correction

- Sentence Segmentation

### 5. Part-of-Speech (POS) Tagging in NLP
**Part-of-speech tagging** is the process of assigning grammatical categories (like noun, verb, adjective) to each word in a sentence. It‚Äôs essential for understanding sentence structure and meaning.

In this project, we demonstrated three types of POS tagging:
‚úÖ **Rule-Based/Statistical Tagging (NLTK)**
We used NLTK's `pos_tag()` which combines handcrafted rules and statistical probabilities.
```python
from nltk import pos_tag, word_tokenize
pos_tag(word_tokenize("The quick brown fox jumps over the lazy dog"))
```

| **Word**  | **POS Tag** | **Meaning**             |
| --------- | ----------- | ----------------------- |
| The       |  DT         | Determiner              |
| quick     |  JJ         | Adjective               |
| brown     |  NN ‚ùå     | Noun (should be ADJ)    |
| fox       |  NN         | Noun                    |
| jumps     |  VBZ        | Verb (3rd person sing.) |
| over      |  IN         | Preposition             |
| the       |  DT         | Determiner              |
| lazy      |  JJ         | Adjective               |
| dog       |  NN         | Noun                    |


üìä **Summary Comparison**:

| Model     | Strengths                      | Weaknesses                             |
| --------- | ------------------------------ | -------------------------------------- |
| **NLTK**  | Fast, simple, rule/statistical | Lacks deep context (e.g., ‚Äúbrown‚Äù)     |
| **spaCy** | Neural, context-aware          | Less accurate on very complex syntax   |
| **BERT**  | Deep context, phrase-sensitive | Slower, more complex, may merge tokens |


üîç Issue: "brown" is misclassified as a noun (NN) instead of an adjective (JJ). This is a known limitation of rule-based systems that lack context understanding.
### 7. What are the differences between rule-based, statistical-based, and neural-based approaches in NLP?
- **Rule-Based Approach:**

    - Uses manually crafted linguistic rules.

    - Highly interpretable, but hard to scale.

    - Good for simple patterns (e.g., regex email extraction).

- **Statistical-Based Approach:**

    - Learns from statistical patterns in labeled data.
    - Example: Naive Bayes for sentiment classification.
    - Flexible and scalable with enough data.

- **Neural-Based Approach:**

    - Uses deep learning models like BERT, GPT.
    - Learns hierarchical language features automatically.
    - High performance, especially with large datasets.

### 8. NLTK: Natural Language Toolkit
#### üì¶ What is NLTK?
NLTK stands for **Natural Language Toolkit**. It is a powerful, open-source Python library for Natural Language Processing (NLP). Developed at the University of Pennsylvania, NLTK provides tools and resources for working with human language data (text), making it one of the most widely used libraries in education and research.
- üìå Official Website: https://www.nltk.org
- üêç Language: Python
- üë®‚Äçüè´ Primary Audience: Students, educators, researchers, beginners
#### üìö Educational Focus
NLTK is often called a "teaching toolkit" because:
- It includes **built-in corpora and lexical resources**, like WordNet.
- It comes with **detailed documentation and examples**.
- It‚Äôs ideal for **learning NLP concepts** without needing huge datasets or deep learning infrastructure.

#### üõ†Ô∏è What Can You Do with NLTK?
| Task                         | Description                                  |
| ---------------------------- | -------------------------------------------- |
| **Tokenization**             | Split text into words or sentences           |
| **Stemming & Lemmatization** | Reduce words to their root form              |
| **Part-of-Speech Tagging**   | Identify nouns, verbs, adjectives, etc.      |
| **Parsing & Syntax Trees**   | Understand grammar and structure             |
| **Named Entity Recognition** | Detect entities like names, dates, locations |
| **Corpora Access**           | Load sample datasets like Gutenberg, Brown   |
| **Language Modeling**        | Create n-gram models and generate text       |
| **Text Classification**      | Build Naive Bayes or other classifiers       |

#### ‚úÖ When to Use NLTK
| Use NLTK If...                           | Consider Alternatives If...                          |
| ---------------------------------------- | ---------------------------------------------------- |
| You are **learning** NLP or linguistics  | You need **high performance** at scale (e.g., spaCy) |
| You want to **prototype quickly**        | You need **neural embeddings or transformers**       |
| You need access to **classic NLP tools** | You want **modern, deep-learning-based NLP**         |

### 9. What is the **Bag-of-Words (BoW)** Model? 
BoW is one of the simplest and most widely used techniques to convert text into numerical feature vectors.
How It Works (Step-by-Step):
- **Tokenization**:
Break the sentence into words (tokens).
Example:
```python
"I love apples." ‚Üí ['I', 'love', 'apples']
```
- **Vocabulary Creation**:
Collect all unique words across all documents.
Example:
```python
["I love apples.", "I love mangoes too."] ‚Üí Vocabulary: ['I', 'love', 'apples', 'mangoes', 'too']
```
- **Vectorization**:
Turn each document into a vector of word counts from the vocabulary.
Each dimension of the vector corresponds to a word in the vocabulary.
Example: 
```python
Doc1: "I love apples."   ‚Üí [1, 1, 1, 0, 0]
Doc2: "I love mangoes too." ‚Üí [1, 1, 0, 1, 1]

```
- **Why Use BoW?**:
BoW transforms human language into a form that machine learning models can understand.

### 10. What are the different types of parsing in NLP?
Constituency Parsing, Dependency Parsing, Top-down Parsing, Bottom-up Parsing
- **Constituency Parsing:**
Constituency parsing breaks down a sentence into nested phrases (or constituents) using a context-free grammar (CFG). The output is a tree where:
    - Leaf nodes = actual words
    - Internal nodes = grammatical phrases (like NP, VP)

```yaml
S
‚îú‚îÄ‚îÄ NP (Noun Phrase)
‚îÇ   ‚îú‚îÄ‚îÄ DT: The
‚îÇ   ‚îî‚îÄ‚îÄ NN: cat
‚îú‚îÄ‚îÄ VP (Verb Phrase)
‚îÇ   ‚îú‚îÄ‚îÄ VBD: sat
‚îÇ   ‚îî‚îÄ‚îÄ PP (Prepositional Phrase)
‚îÇ       ‚îú‚îÄ‚îÄ IN: on
‚îÇ       ‚îî‚îÄ‚îÄ NP
‚îÇ           ‚îú‚îÄ‚îÄ DT: the
‚îÇ           ‚îî‚îÄ‚îÄ NN: mat

```

- **Dependency Parsing:**
Dependency parsing builds a directed graph where:

Each word in a sentence depends on another word

It shows grammatical relations (like subject ‚Üí verb, verb ‚Üí object)


| **Word** | **Head** | **Relation**                        |
|----------|----------|-------------------------------------|
| The      | cat      | determiner (det)                    |
| cat      | sat      | nominal subject (nsubj)             |
| sat      | ROOT     | main verb                           |
| on       | sat      | preposition (prep)                  |
| the      | mat      | determiner                          |
| mat      | on       | object of preposition (pobj)        |

- **Top-Down Parsing**:
Top-down parsing starts at the root of the parse tree (S = Sentence) and recursively applies grammar rules to predict what should come next (i.e., expand S ‚Üí NP + VP).
Starts at root ‚Üí breaks into sub-parts recursively

Example:
```mathematica
S ‚Üí NP VP
NP ‚Üí Det N
VP ‚Üí V PP
PP ‚Üí P NP

```
```yaml
S
‚îú‚îÄ‚îÄ NP
‚îÇ   ‚îú‚îÄ‚îÄ Det: The
‚îÇ   ‚îî‚îÄ‚îÄ N: cat
‚îî‚îÄ‚îÄ VP
    ‚îú‚îÄ‚îÄ V: sat
    ‚îî‚îÄ‚îÄ PP
        ‚îú‚îÄ‚îÄ P: on
        ‚îî‚îÄ‚îÄ NP
            ‚îú‚îÄ‚îÄ Det: the
            ‚îî‚îÄ‚îÄ N: mat

```

### 11. What is TF-IDF in NLP?
**TF-IDF** stands for:

- TF: Term Frequency

- IDF: Inverse Document Frequency

TF-IDF is a way to numerically represent text data ‚Äî used widely in Natural Language Processing (NLP) to determine how important a word is in a document relative to a collection (corpus) of documents.

#### What is Term Frequency (TF)?
TF measures how often a word appears in a document.
- Formula:
**TF(t, d)** = (Number of times term *t* appears in document *d*) / (Total number of terms in document *d*)

#### What is **Inverse Document Frequency (IDF)?**
IDF measures how unique or rare a word is across **all documents** in the corpus.
- Formula:
**IDF(t)** = log( *N* / (1 + *df(t)*) )
Where:
- *N* = Total number of documents  
- *df(t)* = Number of documents containing term *t*

#### üßæ Example:
If "cat" appears in 2 out of 3 documents:
**IDF("cat")** = log( 3 / (1 + 2) ) = log(1) = **0**

> üî∏ So common words get **low IDF scores**, while rare ones get **high scores**.

#### üß† How Is TF-IDF Used in NLP?
TF-IDF is used to convert text into numbers that can be used for:

- üß™ Text classification (e.g., spam detection)
- üóÇ Information retrieval (e.g., search engines)
- üìä Clustering or topic modeling
- ü§ñ Machine learning models that take numerical input

#### üßÆ TF-IDF Score = TF √ó IDF
Words that appear often in a document but rarely elsewhere get a high score.

#### üß† Intuition:
Words like "the" and "is" appear everywhere ‚Üí low IDF ‚Üí low TF-IDF.

Words like "machine", "neural" appear in some docs but not all ‚Üí higher IDF ‚Üí useful for distinguishing docs.

#### üßÆ TF-IDF Score = TF √ó IDF
Words that appear often in a document but rarely elsewhere get a high score.

### 12. Machine Learning Algorithms used in NLP

#### Naive Bayes
- **Type**: Probabilistic classifier based on Bayes' Theorem.
- How it works: Assumes features (like words) are independent. Calculates the probability that a given text belongs to a class (like spam or not spam).
- **Common Use**: Text classification, spam detection, sentiment analysis.
- **Pros**:
    - Simple to implement and interpret
    - Very fast even with large feature sets
    - Performs surprisingly well on small datasets
- **Cons**:
    - **Unrealistic independence assumption** ‚Äî words in natural language are highly dependent on context.
    - Can be biased if a word never appeared in training for a class (though smoothing helps).

#### Support Vector Machines (SVM)
- **Type**: Supervised learning algorithm.
- **How it works**: Finds a hyperplane that best separates data into classes in a high-dimensional space.
- **Common Use**: Text classification, NER (Named Entity Recognition), sentiment analysis.
- **Pros**: 
    - Handles high-dimensional spaces well (like TF-IDF vectors).
    - Effective when the number of samples is small compared to features.
    - With kernel tricks, it can model complex boundaries.

- **Cons**:
    - **Not probabilistic** ‚Äî only gives class decision, not confidence (unless calibrated).
    - Can be slow on very large datasets.
    - Hard to interpret directly (no clear tree or rules).

- **Based on the demo**:
    - Used `SVC` with a linear kernel and TfidfVectorizer.
    - All samples became support vectors ‚Äî common when you have very few, diverse inputs.
    - The model predicted `NLP is bad` as positive, because `NLP` had stronger TF-IDF weights than `bad` and was associated with positive classes in training.

#### Decision Trees
- **Type**: Rule-based supervised learning model.
- **How it works**: Builds a tree structure where each internal node is a decision on a feature, and each leaf node is a class label.
- **Common Use**: Information extraction, binary text classification.
- **Pros**: 
    - Highly interpretable ‚Äî you can visualize and understand decisions.
    - Can handle mixed feature types (text + numeric).
- **Cons**:
    - Prone to overfitting, especially on small datasets.
    - Less robust to small variations in data (a small change can alter the tree structure drastically).
- **Based on the demo**:
    - You trained a DecisionTreeClassifier using CountVectorizer.
    - Your model produced a rule like:

    ```cpp
        if "nlp" appears more than 0.5 ‚Üí class 1 (positive)
        else ‚Üí class 0 (negative)
    ```
    - Simple and interpretable, but fragile ‚Äî e.g., `"Spam is amazing"` could confuse it since "spam" = negative but "amazing" = positive.


#### Random Forests
- **Type**: Ensemble learning (collection of decision trees).
- **How it works**: Builds multiple decision trees and averages their results to improve accuracy and reduce overfitting.
- **Common Use**: Named entity recognition, sentiment classification.
- **Pros**: More accurate and stable than a single decision tree.

#### Transfomer
- **Type**: Attention-based deep learning architecture.
- **How it works**: Uses self-attention to weigh the importance of each word in a sentence relative to others.
- **Common Use**: BERT, GPT, and similar models for classification, translation, Q&A, summarization.
- **Pros**: State-of-the-art results; captures long-range dependencies better than LSTMs/RNNs.

### 13. Transfomers
#### What is transformer in NLP
The Transformer is a deep learning architecture introduced in the paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762). It has become the foundation of modern NLP models like BERT, GPT, and RoBERTa.

It revolutionized NLP by replacing traditional RNNs and CNNs with **self-attention mechanisms**. Unlike RNNs or LSTMs, which process sequences token by token, Transformers use self-attention to analyze an entire sequence in parallel. This allows them to capture global context, making them faster and more effective for long sequences.

#### ‚úÖ Key Features of Transformer:
- **Parallelization**: Processes sequences in parallel using attention, not step-by-step like RNNs
- **Long-Range Dependency Handling**: Self-attention lets the model learn relationships between distant words
- **State-of-the-Art Performance**: Used in models like BERT, GPT, T5, achieving top results in NLP tasks

#### üß† How the Transformer Works (Simplified Overview):
1. **Token Embedding**
Each word is converted into a dense vector using an `nn.Embedding` layer.
2. **Positional Encoding**
Since Transformers don't have sequence order by default, positional encodings (sinusoidal functions) are added to the embeddings to represent the position of each token in the sentence.
$PE(pos, i) = \sin\left(\frac{pos}{10000^{\frac{2i}{d_{model}}}}\right)$

3. **Self-Attention**
The self-attention mechanism is the core of the Transformer architecture. It allows the model to weigh the importance of different words in a sequence when encoding each word ‚Äî enabling the model to understand context from surrounding tokens.

Self-attention computes how much each word in a sentence should **attend to (focus on)** every other word ‚Äî helping models understand context, dependencies, and meaning.
For a word `ùëû`, self-attention is computed as:
    Attention(q, k, v) = softmax((q ¬∑ k·µÄ) / ‚àöd‚Çñ) ¬∑ v
Where:
- `q`, `k`, `v`: query, key, and value vectors
- `d‚Çñ`: dimension of keys (for scaling)
- The result is a weighted sum of value vectors, based on how relevant each word is to the query

Intuition behind q, k v:
- Q(Query): What you‚Äôre **looking for** (e.g., ‚ÄúWho should I pay attention to?‚Äù)
- K(Key): What each other word offers as an identity (e.g., ‚ÄúWhat am I about?‚Äù)
- V(Value): The actual content/information each word holds (e.g., ‚ÄúHere‚Äôs my info‚Äù)

4. **Multi-Head Attention**
Instead of computing a single attention score, multiple attention heads capture different relationships in parallel, and their results are concatenated and projected back to the embedding space.
5. **Feedforward Network (FFN)**
After self-attention, each token's vector goes through a fully connected feedforward layer with non-linearity.
6. **Residual Connections + Layer Normalization**
Each block includes skip connections (residuals) and layer normalization to stabilize training.

#### üìö Real-World Transformer-Based Models
| Model          | Architecture    | Use Case                   |
| -------------- | --------------- | -------------------------- |
| **BERT**       | Encoder         | Text classification, QA    |
| **GPT**        | Decoder         | Text generation, chat      |
| **T5**         | Encoder-Decoder | Translation, summarization |
| **DistilBERT** | Compressed BERT | Faster inference           |

#### Interview Questions:
1. What is the purpose of the **multi-head attention mechanism** in Transformers?

Multi-head attention means using **multiple self-attention layers (heads)** in parallel. Each head learns to focus on different aspects of the input ‚Äî syntax, semantics, entity relationships, etc.
Code: 
```python
    Q = W_q(x)  # [B, T, D]
    K = W_k(x)
    V = W_v(x)
```
```python
    Q = W_q(x).reshape(B, T, H, D_h).transpose(1, 2)  # [B, H, T, D_h]
    K = W_k(x).reshape(B, T, H, D_h).transpose(1, 2)
    V = W_v(x).reshape(B, T, H, D_h).transpose(1, 2)
```
2. Why do Q and K use different weight matrices? Why not just use the same input for dot product?

Q (query) and K (key) play different semantic roles. Using the same weights removes asymmetry and reduces representational capacity. Dot product of the same vector (e.g., Q¬∑Q·µÄ) only captures self-similarity and lacks contextual interactions.

3. Why does Transformer use multiplication (dot product) for attention instead of addition?
Dot product attention is computationally more efficient and parallelizable using matrix operations. Additive attention (used in RNNs) is harder to scale and slower. Dot product attention performs better with multi-head design on large datasets.

4. Why do we scale the dot product by ‚àödk before softmax?
To prevent large dot product values from pushing the softmax into regions with small gradients, leading to vanishing updates. Scaling by ‚àödk keeps the variance of the dot product stable and ensures effective learning.

13. What is BatchNorm? Advantages and disadvantages?
- **BatchNorm** normalizes each feature (each column) across a batch of inputs:
$$
\hat{x} = \frac{x - \mu_{\text{batch}}}{\sqrt{\sigma^2_{\text{batch}} + \epsilon}}
$$
- ‚úÖ Pros:
    - Reduces internal covariate shift
    - Can accelerate the training process
- ‚ùå Cons:
    - Sensitive to batch size, especially unstable in NLP tasks
    - Poor at handling long sequences
    - Requires fixed statistics during inference, which affects models that need to adapt dynamically

16. What‚Äôs the difference between encoder and decoder attention in Transformer?
| Module        | Source of Q / K / V                                                         | Type                  |
| ------------- | --------------------------------------------------------------------------- | --------------------- |
| **Encoder**   | Q, K, V all come from the encoder input (self-attention)                    | Self-Attention        |
| **Decoder-1** | Q, K, V all come from the already generated part of the decoder (with mask) | Masked Self-Attention |
| **Decoder-2** | Q comes from the decoder, K and V come from the encoder output              | Cross-Attention       |

üîπ Notes:
- The first attention sublayer in the decoder is Masked Multi-Head Self-Attention, which prevents information leakage from future tokens (auto-regressive modeling).

- The second attention sublayer in the decoder is Cross-Attention, which integrates the information encoded by the encoder.

<!-- word embeddings -->
### 14. Word Embeddings
Word embeddings are **dense vector representations** of words in a continuous space, where semantically or syntactically similar words are placed closer together. Unlike one-hot encoding or BoW (Bag of Words), embeddings capture contextual relationships and meaning.

#### üîß How it Works (Demo Summary)
In this project, we used `gensim.downloader` to load the pre-trained word2vec-google-news-300 model and explored embeddings with:
```python
model = api.load("word2vec-google-news-300")
```
This model maps each word to a 300-dimensional vector trained on Google News.

#### ‚úÖ What We Explored
- **Semantic Similarity:**:
    ```text
        Similarity between 'king' and 'queen': 0.6511
    ```
- **Syntactic Analogy**:
    ```text
    'king' - 'man' + 'woman' = 'queen' (score: 0.7118)
    ```
- **Nearest Neighbors**:
    ```text
        Words most similar to 'apple':
    apples, pear, fruit, berry, pears

    ```
- **Out-of-Vocabulary Detection:**:
    ```text
        datascience is OOV (Out-of-Vocabulary)
    ```
#### Several popular algorithms to generate word embeddings:
There are several widely used algorithms to generate word embeddings. These methods differ in how they capture the meaning and context of words:

1. **word2vec**: 
A shallow neural network that learns word associations based on local context in a corpus. It offers two architectures:
- **CBOW**(Continuous Bag of Words): Predicts a target word from its surrounding context.
- **Skip-gram**: Predicts surrounding words from a target word.
                These embeddings are **static** (same vector regardless of sentence).

2. **GloVe (Global Vectors)**:
Constructs word embeddings by factoring a word co-occurrence matrix across the entire corpus. It captures **global statistical information** and produces **static embeddings** like word2vec, but based on overall co-occurrence patterns.
- **FastText**: Extends word2vec by including subword (character n-gram) information to better handle rare and misspelled words.
- **ELMo**: Produces context-dependent embeddings using a bidirectional LSTM trained on a language modeling objective.
#### üß† Why Word Embeddings Matter
- They **capture meaning** more effectively than BoW or TF-IDF.

- Enable **vector operations** (e.g., analogies).

- Useful for downstream tasks: classification, translation, question answering, etc.    

#### üì¶ Tools Used
- `gensim` for pretrained embedding loading.
- `contextlib.redirect_stdout` to write output to a file.
- `redirect_stderr` to silence progress bar clutter.

<!-- positional encoding -->
### 15. Positional Encoding

#### üì¶ What Is Positional Encoding?
**Positional Encoding** gives each token a sense of its position in the sequence.
This means they **don't inherently understand the order** of words.
#### üîç Why Do We Need Positional Encoding?
Transformers process input **in parallel**, not sequentially like RNNs.
This means they don't inherently understand the order of words.

There are several ways to compute the position encoding:
1. **Sinusoidal Positional Encoding** (from the original Transformer paper)
$$
PE_{(pos,2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)
$$

$$
PE_{(pos,2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)
$$
- `pos` is the position (0 to max_seq_len)
- `i` is the dimension index
- `d_model` is the embedding size

2. **Learnable Positional Embedding**

Instead of using sin/cos, we just define a **trainable embedding matrix**:
$$
\text{PositionEmbedding} = \text{Embedding(position\_id)}
$$
Each position has a corresponding learnable vector, like word embeddings.

üìà **Properties**:
- Flexible, task-adaptive
- Requires learning
- **Doesn't generalize** to longer sequences than seen during training

3. **Relative Positional Encoding**

Used in models like **Transformer-XL**, **T5**, **DeBERTa**.
Instead of absolute position (like ‚ÄúI'm token 4‚Äù), it encodes the **relative distance between tokens**:
"How far is token A from token B?"
This allows better generalization for tasks like translation, where relative structure is more important than absolute positions.

4. **Rotary Positional Embedding (RoPE)**‚Äî Used in GPT-3.5/GPT-4
- **Intuition:**
    - Rotate each query/key vector by an angle proportional to its position
    - Let attention be **implicitly aware of position differences**

- **Benefits:**
    - Injects **relative and absolute** position info into attention
    - More efficient for long contexts
    - Better generalization to longer sequences

### 16. BERT
#### 16.1 üß† What is BERT?
BERT (Bidirectional Encoder Representations from Transformers) is a **pretrained language model** developed by Google in 2018. 
It is based on the **Transformer encoder architecture** and is trained to understand the **context of a word in a sentence by looking both left and right (bidirectional)**.

#### 16.2 üìå Key Characteristics of BERT:
- 1. **Only uses the Transformer encoder** (not the decoder)
- 2. **Pretrained on a massive text corpus** (Wikipedia + BookCorpus)
- 3. **Fine-tunable** on downstream tasks: sentiment analysis, question answering, NER, etc.
- 4. Uses **[MASK]** token to train via **Masked Language Modeling (MLM)**

#### 16.3 ‚öôÔ∏è How BERT Works ‚Äì Step by Step
- 1. **Transformer Encoder Architecture**:
BERT uses only the **encoder part** of the Transformer (from the original "Attention is All You Need" paper). The encoder focuses on capturing contextual relationships between words in a sentence.
- 2. **Bidirectional Attention**:
Unlike earlier models like GPT (which are left-to-right), BERT reads text in **both directions**.
**Example**:
- Sentence: `"The bank raised the interest rate."`
    - Left-to-right model: sees ‚ÄúThe bank raised the...‚Äù
    - BERT: sees both ‚ÄúThe bank raised...‚Äù and ‚Äú...the interest rate.‚Äù simultaneously.

This enables BERT to **understand context deeply**, especially for polysemous words (words with multiple meanings).
- 3. **Input Representation**:
    - Each input to BERT is a combination of:
        - **Token embeddings** (from WordPiece tokenizer)
        - **Segment embeddings** (to differentiate Sentence A from Sentence B)
        - **Position embeddings** (to capture order)

- 4. **Pretraining Tasks**:
    - BERT is pretrained on large corpora (like Wikipedia and BooksCorpus) using two unsupervised tasks:
    - **(a) Masked Language Modeling (MLM)**:
        - Randomly masks 15% of input tokens
        - Model must predict the masked words using context from both sides
    - **(b) Next Sentence Prediction (NSP)**:
        - Given a pair of sentences, predict if the second sentence logically follows the first.
        ```text
            Sentence A: "She opened the book."
            Sentence B: "It was her favorite novel." ‚Üí Label: True

            Sentence B: "He went to the store." ‚Üí Label: False
        ```
- 5. **Fine-tuning for Downstream Tasks**
    - Once pretrained, BERT can be fine-tuned by adding a task-specific layer on top (e.g., classification head) and training on a smaller dataset.

    - Example fine-tuning:

        - Add a classification head on top of [CLS] token's representation.

        - Fine-tune entire model + new head using task-specific labeled data.

#### üîç 16.4 Summary Table
| Component         | Details                                                         |
| ----------------- | --------------------------------------------------------------- |
| Architecture      | Transformer Encoder (only)                                      |
| Context           | **Bidirectional**                                               |
| Tokenization      | WordPiece with `[CLS]`, `[SEP]`, and `[MASK]` tokens            |
| Pretraining Tasks | Masked Language Modeling (MLM), Next Sentence Prediction (NSP)  |
| Fine-tuning       | Add task-specific layers; train entire model                    |
| Strengths         | Deep context, state-of-the-art accuracy, easy transfer learning |
| Limitations       | Large size, expensive to train from scratch                     |

### 17. Gensim
#### üì¶ What is Gensim?
Gensim stands for **‚ÄúGenerate Similar‚Äù**. It is an open-source Python library developed by Radim ≈òeh≈Ø≈ôek and is used to process and analyze large-scale semantic textual data using unsupervised algorithms.

#### ‚úÖ Key Characteristics:
- Efficient in handling **large corpora** (even bigger than memory)
- Focused on **vector space modeling**, especially Word2Vec, Doc2Vec, and Topic Modeling
- Supports **streaming** and **incremental training**

#### üéØ Main Purposes of Gensim
- Learn word relationships: Word2Vec, FastText
- Represent documents as vectors: Doc2Vec, TF-IDF, Bag-of-Words
- Find topics in a corpus: LDA (Latent Dirichlet Allocation), HDP
- Compute similarity between docs: Cosine similarity with vector models
- Handle large-scale datasets: Memory-efficient streaming API

####  Core Features and Algorithms
- Bag-of-Words (BoW)
- TF-IDF (Term Frequency‚ÄìInverse Document Frequency)
- Word2Vec
- Doc2Vec
- Topic Modeling (LDA)

### 18. OOV(Out-of-Vocabulary)Words
#### üß† What Are OOV Words?
#### ‚úÖ Techniques to Handle OOV Words
- **Character-Level Models**:
    - **What It Is**: Character-level models represent text **one character at a time,** instead of by full words or subwords.
    - **How It Works**: 
        - Input: `"xenobot"` ‚Üí `["x", "e", "n", "o", "b", "o", "t"]`
        - The model learns **character sequences** rather than whole-word meanings.
        - It can generalize to OOV words **if they contain familiar patterns**.
    - **‚ùå Limitations:**:
        - Slower to train and decode.
        - May lose long-range semantic information.
- **Subword Tokenization (BPE, WordPiece)**:
    - Instead of full words, text is broken into **frequent subword units** using algorithms like:
        - **Byte Pair Encoding** (BPE): Used in GPT
        - **WordPiece**: Used in BERT
        - **Unigram**: Used in SentencePiece and T5
    - **üõ†Ô∏è How It Works:**
        - `"xenobot"` ‚Üí `["x", "##eno", "##bot"]`
        - These subwords exist in the vocabulary even if the full word doesn't.
    - **üß† Why It Helps:**:
        - Allows the model to handle **unknown or compound words**.
        - Prevents vocabulary from growing too large.
        - Most modern transformer models use this.
- **Unknown Token ([UNK])**:
    - **üìå What It Is**:
        - Use a special placeholder token like `[UNK]` to represent any OOV word.
    - **üõ†Ô∏è How It Works:**: 
        - `"xenobot" ‚Üí [UNK]`
        - During training or inference, the model substitutes OOVs with this token.
    - **üß† Why It Helps**:
        - Simple and prevents errors/crashes.
        - Still allows the model to make a **best guess** using surrounding context.
    - **‚ùå Limitations**:
        - Loses all **semantic meaning** of the unknown word.
        - Can lead to poor or generic predictions.
- **External Knowledge**:
    - **üìå What It Is**: Use external sources like:
        - **WordNet**, **Wikipedia**, or knowledge graphs (e.g., DBpedia, ConceptNet)
        - Custom dictionaries or glossaries
    - **üõ†Ô∏è How It Works:**:
        - When encountering `"neuralink"`, look up its meaning externally.
        - Integrate this information into model inference.
    - **üß† Why It Helps:**:
        - Useful in domains like **biomedicine**, **law**, or **education**, where models often miss terms.
        - Supports **explainability** and human-in-the-loop systems.
    - **‚ùå Limitations**:
        - Requires access to a live or pre-built knowledge source.
        - Integration with models can be complex.
- **Fine-Tuning with OOV Data**:
    - **What It Is**: Retrain or fine-tune a pre-trained model with **examples that contain OOV words**, so it learns their usage.
    - **üõ†Ô∏è How It Works**:
        - Use domain-specific text with new vocabulary.
        - Fine-tune a language model like BERT or GPT on this data.
    - **üß† Why It Helps:**:
        - The model **learns the context and meaning** of new words.
        - Becomes more specialized and accurate for the domain.
    - **‚úÖ Real-World Use Cases:**:
        - **Medical NLP** (new drugs, diseases)
        - **Tech support bots** (new products)
        - **ChatGPT fine-tuned for company-specific terminology**

### 19. Machine Translation
Machine translation is the process of automatically translating text or speech from one language to another using a computer or machine learning model.

There are three techniques for machine translation:
#### 1. Rule-Based Machine Translation (RBMT)
- üìå Concept:
RBMT systems translate text based on **manually defined linguistic rules** and **bilingual dictionaries**. It operates by analyzing the grammatical structure of the source sentence and applying syntactic and semantic rules to generate a grammatically correct target sentence.
-  In Practice:
```python
# Dictionary-based simulation
dictionary = {"hello": "bonjour", "world": "monde"}
``` 
####  2. Statistical Machine Translation (SMT)
- üìå Concept:
SMT systems translate by using probability and statistics derived from large aligned bilingual corpora (e.g., English-French sentence pairs). These systems learn patterns based on how often words or phrases co-occur.

- üß†**How it works**:
    - Align source and target sentences
    - Learn probability distributions of phrase translations
    - Use language models to ensure fluency
    - Translation = most probable target sentence given source

- üí°**Key Concepts**:
    - Phrase-based SMT: Translates phrases rather than individual words
    - Language Model (LM): Helps ensure fluent output (e.g., ‚Äúa red car‚Äù is more likely than ‚Äúa car red‚Äù)

- ‚úÖ **Pros**:
    - Data-driven, adaptable to new languages with corpora
    - Outperforms RBMT in general cases

- ‚ùå **Cons**:
    - Still **rule-blind** (just statistical co-occurrence)
    - Needs huge datasets
    - Outputs often lack fluency or naturalness
    - Cannot easily handle long-range dependencies

#### üß† 3. Neural Machine Translation (NMT)
- üìå **Concept**:
NMT uses **deep learning models**, particularly **sequence-to-sequence architectures** and **Transformers**, to learn to translate from large datasets. It captures meaning and context more holistically.
- üß† **How it works**:
    - Input sentence ‚Üí tokenized (e.g., subwords)
    - Encoder processes sentence ‚Üí creates vector representation
    - Decoder generates translation token-by-token
    - Attention mechanism allows model to focus on relevant parts of input during decoding
- üî¨ **Key Tech**:
    - **Transformer** architecture (BERT, GPT, T5, MarianMT)
    - **Word embeddings** to capture meaning
    - **Attention** to focus on the right parts of input
- üîß **Tools**:
    - Hugging Face Transformers (e.g., `Helsinki-NLP/opus-mt-en-fr`)
    - Fairseq, OpenNMT, Tensor2Tensor
- ‚úÖ **Pros**:
    - Best-in-class fluency and accuracy

    - Captures long-distance dependencies

    - Learns language patterns automatically
- ‚ùå **Cons**:
    - Requires a lot of data and compute to train

    - Can be opaque (not easily interpretable)

    - Sometimes hallucinates (outputs fluent but incorrect translations)

### 20. What is Sequence Labeling?
#### 2. Named Entity Recognition (NER)
Recognizes and classifies **named entities** (real-world objects) into predefined categories:
#### 3. Chunking (Shallow Parsing)
Groups words into syntactic "chunks" like **noun phrases (NP)** or **verb phrases (VP)**.
üîπ **Purpose**: Captures structure between words (e.g., ‚ÄúThe quick brown fox‚Äù = NP).

### 21. LayerNorm VS Batch Norm
#### Definition
- **Batch Normalization** is a technique that normalizes the inputs of a layer across the batch dimension, i.e., for each feature independently across all examples in a batch. It stabilizes and accelerates training by reducing internal covariate shift.
- **Layer Normalization** normalizes the inputs across all features in a single sample. Unlike BatchNorm, it does not depend on the batch and works well for sequential and language data.

#### Pros:
- **Batch Normalization**:
    - Speeds up convergence

    - Reduces internal covariate shift

    - Acts as regularizer
- **Layer Normalization**:
    - Independent of batch size

    - Works on variable-length inputs

    - Suitable for autoregressive and sequential models
#### Cons:
- **Batch Normalization**:
    - Sensitive to batch size

    - Less effective in NLP or variable-length sequences

    - Requires batch statistics (inference needs stored values)

- **Layer Normalization**:
    - Slightly more computational overhead
    - No benefit from batch-level regularization

#### ‚úÖ Conceptual Differences
| Feature                  | Batch Normalization (BatchNorm)          | Layer Normalization (LayerNorm)          |
| ------------------------ | ---------------------------------------- | ---------------------------------------- |
| **Normalization Axis**   | Across **batch dimension**               | Across **feature dimension** per sample  |
| **Best For**             | Vision / CNN, large batches              | NLP / RNN / Transformer, small batches   |
| **Depends on Batch?**    | ‚úÖ Yes                                    | ‚ùå No                                     |
| **Formula**              | Normalize over `(N)` samples per feature | Normalize over `(D)` features per sample |
| **Training Sensitivity** | Sensitive to batch size                  | Stable even with batch size = 1          |

#### Batch Norm Output Explanation
üîç What happened?
- BatchNorm normalized each feature column across the entire batch.

- That means it looks at all 6 rows (2 batches √ó 3 sequences) for each of the 4 features.

üîç Observation:
- All values in the same column (feature) become identical **across each row** (because they‚Äôre normalized per feature).

- That‚Äôs why each entire row becomes uniform like `[-1.4638, -1.4638, -1.4638, -1.4638]`


## üß© Python Dependencies

Below are the main libraries required to run this NLP demo project:

| Package            | Version    | Description                                                                   |
|--------------------|------------|-------------------------------------------------------------------------------|
| `nltk`             |  3.8.1     | Natural Language Toolkit for classic NLP tasks like tokenization and POS      |
| `spacy`            |  3.7.2     | Industrial-strength NLP library used with `benepar` for parsing               |
| `textblob`         |  0.17.1    | Simple NLP API for sentiment analysis and noun phrase extraction              |
| `beautifulsoup4`   |  4.12.3    | For parsing HTML/XML data                                                     |
| `transformers`     |  4.39.3    | Hugging Face Transformers library (e.g., BERT, GPT, RoBERTa)                  |
| `torch`            |  >=2.0.0   | PyTorch backend, required for transformer models                              |
| `benepar`          |  0.2.0     | Berkeley Neural Parser, used for constituency parsing with spaCy              |
| `scikit-learn	`    |  1.4.2     | Machine learning toolkit; used here for `TfidfVectorizer` and classifiers     |
| `pandas`           |  2.2.2     | Data analysis and manipulation; used for displaying TF-IDF results            |
| `gensim`           |  4.3.2     | Topic modeling and word embeddings (Word2Vec, FastText, GloVe via downloader) |
| `scipy`            |	1.12.0    | Scientific computing; used for similarity metrics and matrix operations       |
| `smart_open`       |	>=6.3.0	  | Streams pretrained GloVe and other large files in gensim                      |

## Setup
1. clone the repository
```sh
git clone https://github.com/Ranxin2023/NLP_Demo
```
2. install the requirement
```sh
pip install -r requirements.txt

```
3. Open a python shell
```sh
python -m nltk.downloader all
python -m spacy download en_core_web_sm
python -m benepar.download_en3
``` 
4. Open main.py, choose the demo function you want to run, then run
```sh
python main.py
```