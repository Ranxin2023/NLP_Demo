# NLPDEMO: Natural Language Processing Hands-On Project
## Intruduction
This project is a hands-on demo of various Natural Language Processing (NLP) techniques, organized in modular Python scripts. It walks through fundamental concepts such as preprocessing, normalization, text classification, lemmatization, different NLP model approaches, and more.

## Project Structure
```graphql
NLPMDEMO/
â”œâ”€â”€ datasets/
â”‚   â””â”€â”€ text_preprocessing.html               # HTML resource for preprocessing
â”œâ”€â”€ demo_code/                                # All demo Python scripts
â”‚   â”œâ”€â”€ __pycache__/
â”‚   â”œâ”€â”€ cos_similarity_examples.py            # Cosine similarity demo
â”‚   â”œâ”€â”€ nlp_challenges.py                     # Challenges in NLP
â”‚   â”œâ”€â”€ nlp_tasks.py                          # Common NLP tasks
â”‚   â”œâ”€â”€ nltk_demo.py                          # NLTK-based parsing demo
â”‚   â”œâ”€â”€ parsing_demo.py                       # Constituency & dependency parsing
â”‚   â”œâ”€â”€ preprocessing.py                      # Text preprocessing pipeline
â”‚   â”œâ”€â”€ text_normalization.py                 # Normalization: lowercase, stemming, etc.
â”‚   â”œâ”€â”€ tf_idf_demo.py                        # TF-IDF demonstration
â”‚   â””â”€â”€ three_approaches.py                   # Top-down, bottom-up, constituency parsing
â”œâ”€â”€ output_results/                           # Output files from the demos
â”‚   â”œâ”€â”€ cos_similarity_demo.txt
â”‚   â”œâ”€â”€ nlp_challenges.txt
â”‚   â”œâ”€â”€ nlp_tasks.txt
â”‚   â”œâ”€â”€ nltk_demo.txt
â”‚   â”œâ”€â”€ parsing_result.txt
â”‚   â”œâ”€â”€ text_normalization.txt
â”‚   â”œâ”€â”€ text_preprocessing.txt
â”‚   â””â”€â”€ three_approach_result.txt
â”œâ”€â”€ .gitignore                                # Git ignore file
â”œâ”€â”€ main.py                                   # Optional entry point script
â”œâ”€â”€ README.md                                 # Project overview
â””â”€â”€ requirements.txt                          # Python dependencies


```

## NLP Concepts
### 1. What is NLP?
Natural Language Processing (NLP) is a subfield of Artificial Intelligence that focuses on the interaction between computers and human language. It enables machines to understand, interpret, and generate natural language in a way that is meaningful. NLP encompasses tasks such as translation, sentiment analysis, question answering, and more.

### 2. What are the main challenges in NLP?

### 4. What are some common preprocessing techniques used in NLP?
- Tokenization: Splitting text into words or phrases.

- Stop Word Removal: Removing common, less informative words.

- Text Normalization:

    - Lowercasing

    - Lemmatization

    - Stemming

    - Date/Time normalization

- Punctuation/Special Character Removal

- HTML Tag Removal

- Spell Correction

- Sentence Segmentation

### 6. What are the differences between rule-based, statistical-based, and neural-based approaches in NLP?
- **Rule-Based Approach:**

    - Uses manually crafted linguistic rules.

    - Highly interpretable, but hard to scale.

    - Good for simple patterns (e.g., regex email extraction).

- **Statistical-Based Approach:**

    - Learns from statistical patterns in labeled data.

    - Example: Naive Bayes for sentiment classification.

    - Flexible and scalable with enough data.

- **Neural-Based Approach:**

    - Uses deep learning models like BERT, GPT.

    - Learns hierarchical language features automatically.

    - High performance, especially with large datasets.

### 7. What is the **Bag-of-Words (BoW)** Model? 
BoW is one of the simplest and most widely used techniques to convert text into numerical feature vectors.
How It Works (Step-by-Step):
- **Tokenization**:
Break the sentence into words (tokens).
Example:
```python
"I love apples." â†’ ['I', 'love', 'apples']
```
- **Vocabulary Creation**:
Collect all unique words across all documents.
Example:
```python
["I love apples.", "I love mangoes too."] â†’ Vocabulary: ['I', 'love', 'apples', 'mangoes', 'too']
```
- **Vectorization**:
Turn each document into a vector of word counts from the vocabulary.
Each dimension of the vector corresponds to a word in the vocabulary.
Example: 
```python
Doc1: "I love apples."   â†’ [1, 1, 1, 0, 0]
Doc2: "I love mangoes too." â†’ [1, 1, 0, 1, 1]

```
- **Why Use BoW?**:
BoW transforms human language into a form that machine learning models can understand.

### 8. What are the different types of parsing in NLP?
Constituency Parsing, Dependency Parsing, Top-down Parsing, Bottom-up Parsing
- **Constituency Parsing:**
Constituency parsing breaks down a sentence into nested phrases (or constituents) using a context-free grammar (CFG). The output is a tree where:
    - Leaf nodes = actual words
    - Internal nodes = grammatical phrases (like NP, VP)

```yaml
S
â”œâ”€â”€ NP (Noun Phrase)
â”‚   â”œâ”€â”€ DT: The
â”‚   â””â”€â”€ NN: cat
â”œâ”€â”€ VP (Verb Phrase)
â”‚   â”œâ”€â”€ VBD: sat
â”‚   â””â”€â”€ PP (Prepositional Phrase)
â”‚       â”œâ”€â”€ IN: on
â”‚       â””â”€â”€ NP
â”‚           â”œâ”€â”€ DT: the
â”‚           â””â”€â”€ NN: mat

```
- **Dependency Parsing:**
Dependency parsing builds a directed graph where:

Each word in a sentence depends on another word

It shows grammatical relations (like subject â†’ verb, verb â†’ object)


| **Word** | **Head** | **Relation**                        |
|----------|----------|-------------------------------------|
| The      | cat      | determiner (det)                    |
| cat      | sat      | nominal subject (nsubj)             |
| sat      | ROOT     | main verb                           |
| on       | sat      | preposition (prep)                  |
| the      | mat      | determiner                          |
| mat      | on       | object of preposition (pobj)        |


- **Top-Down Parsing**:
Top-down parsing starts at the root of the parse tree (S = Sentence) and recursively applies grammar rules to predict what should come next (i.e., expand S â†’ NP + VP).
Starts at root â†’ breaks into sub-parts recursively

Example:
```mathematica
S â†’ NP VP
NP â†’ Det N
VP â†’ V PP
PP â†’ P NP

```
```yaml
S
â”œâ”€â”€ NP
â”‚   â”œâ”€â”€ Det: The
â”‚   â””â”€â”€ N: cat
â””â”€â”€ VP
    â”œâ”€â”€ V: sat
    â””â”€â”€ PP
        â”œâ”€â”€ P: on
        â””â”€â”€ NP
            â”œâ”€â”€ Det: the
            â””â”€â”€ N: mat

```

### 9. What is TF-IDF in NLP?
**TF-IDF** stands for:

- TF: Term Frequency

- IDF: Inverse Document Frequency

TF-IDF is a way to numerically represent text data â€” used widely in Natural Language Processing (NLP) to determine how important a word is in a document relative to a collection (corpus) of documents.

#### What is Term Frequency (TF)?
TF measures how often a word appears in a document.
- Formula:
**TF(t, d)** = (Number of times term *t* appears in document *d*) / (Total number of terms in document *d*)
#### What is **Inverse Document Frequency (IDF)?**
IDF measures how unique or rare a word is across **all documents** in the corpus.
- Formula:
**IDF(t)** = log( *N* / (1 + *df(t)*) )
Where:
- *N* = Total number of documents  
- *df(t)* = Number of documents containing term *t*
### ðŸ§¾ Example:
If "cat" appears in 2 out of 3 documents:

**IDF("cat")** = log( 3 / (1 + 2) ) = log(1) = **0**

> ðŸ”¸ So common words get **low IDF scores**, while rare ones get **high scores**.
#### ðŸ§  How Is TF-IDF Used in NLP?
TF-IDF is used to convert text into numbers that can be used for:

- ðŸ§ª Text classification (e.g., spam detection)

- ðŸ—‚ Information retrieval (e.g., search engines)

- ðŸ“Š Clustering or topic modeling

- ðŸ¤– Machine learning models that take numerical input
#### ðŸ§® TF-IDF Score = TF Ã— IDF
Words that appear often in a document but rarely elsewhere get a high score.

#### ðŸ§  Intuition:
Words like "the" and "is" appear everywhere â†’ low IDF â†’ low TF-IDF.

Words like "machine", "neural" appear in some docs but not all â†’ higher IDF â†’ useful for distinguishing docs.

ðŸ§® TF-IDF Score = TF Ã— IDF
Words that appear often in a document but rarely elsewhere get a high score.
## ðŸ§© Python Dependencies

Below are the main libraries required to run this NLP demo project:

| Package            | Version     | Description                                                                 |
|--------------------|-------------|-----------------------------------------------------------------------------|
| `nltk`             | 3.8.1       | Natural Language Toolkit for classic NLP tasks like tokenization and POS   |
| `spacy`            | 3.7.2       | Industrial-strength NLP library used with `benepar` for parsing            |
| `textblob`         | 0.17.1      | Simple NLP API for sentiment analysis and noun phrase extraction           |
| `beautifulsoup4`   | 4.12.3      | For parsing HTML/XML data                                                  |
| `transformers`     | 4.39.3      | Hugging Face Transformers library (e.g., BERT, GPT, RoBERTa)               |
| `torch`            | >=2.0.0     | PyTorch backend, required for transformer models                           |
| `benepar`          | 0.2.0       | Berkeley Neural Parser, used for constituency parsing with spaCy           |


## Setup
1. clone the repository
```sh
git clone https://github.com/Ranxin2023/NLP_Demo
```
2. install the requirement
```sh
pip install -r requirements.txt

```
3. Open a python shell
```sh
python -m nltk.downloader all
python -m spacy download en_core_web_sm
python -m benepar.download_en3
``` 
4. Open main.py, choose the demo function you want to run, then run
```sh
python main.py
```