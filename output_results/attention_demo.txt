attention of transformer......
B, T, D for x shape is1, 4, 8
== Single-Head Self-Attention Output ==
tensor([[[-0.5850, -0.3930,  0.0737,  0.1385,  0.2806,  0.5237, -0.4288,
           0.0074],
         [-0.5856, -0.3935,  0.0742,  0.1385,  0.2805,  0.5236, -0.4303,
           0.0058],
         [-0.5867, -0.3950,  0.0716,  0.1375,  0.2815,  0.5253, -0.4318,
           0.0065],
         [-0.5872, -0.3925,  0.0764,  0.1386,  0.2801,  0.5224, -0.4312,
           0.0031]]], grad_fn=<UnsafeViewBackward0>)

== Multi-Head Attention Output ==
tensor([[[ 0.0772, -0.0492, -0.0349,  0.3264, -0.1141,  0.1496,  0.0075,
          -0.2753],
         [ 0.0769, -0.0500, -0.0339,  0.3271, -0.1147,  0.1498,  0.0079,
          -0.2753],
         [ 0.0757, -0.0489, -0.0345,  0.3250, -0.1124,  0.1508,  0.0076,
          -0.2777],
         [ 0.0759, -0.0490, -0.0368,  0.3246, -0.1114,  0.1501,  0.0078,
          -0.2769]]], grad_fn=<ViewBackward0>)

== Multi-Head Attention Weights Shape ==
torch.Size([1, 2, 4, 4])
BERT Layer 0 Head 0 Attention Weights:
 tensor([[0.0919, 0.2086, 0.0697, 0.0806, 0.1075, 0.4416],
        [0.1786, 0.1911, 0.1457, 0.1230, 0.1925, 0.1691],
        [0.1342, 0.1160, 0.1534, 0.2414, 0.1181, 0.2369],
        [0.1073, 0.1511, 0.2138, 0.0881, 0.1583, 0.2812],
        [0.1148, 0.0844, 0.1062, 0.1242, 0.0834, 0.4870],
        [0.2008, 0.1855, 0.1117, 0.0766, 0.1336, 0.2917]],
       grad_fn=<SelectBackward0>)
