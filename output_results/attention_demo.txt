B, T, D for x shape is1, 4, 8
== Single-Head Self-Attention Output ==
tensor([[[ 0.3090,  0.5394,  0.5236,  0.0496,  0.0989,  0.0276,  0.3380,
          -0.3031],
         [ 0.3038,  0.5432,  0.5211,  0.0489,  0.1025,  0.0353,  0.3433,
          -0.3066],
         [ 0.3229,  0.5364,  0.5226,  0.0541,  0.0962,  0.0159,  0.3329,
          -0.2989],
         [ 0.3173,  0.5369,  0.5200,  0.0530,  0.0976,  0.0214,  0.3363,
          -0.2998]]], grad_fn=<UnsafeViewBackward0>)

== Multi-Head Attention Output ==
tensor([[[ 0.2865,  0.0283,  0.3058,  0.2336, -0.1628,  0.2020,  0.0360,
           0.1692],
         [ 0.2847,  0.0321,  0.3034,  0.2343, -0.1616,  0.2035,  0.0353,
           0.1699],
         [ 0.2825,  0.0202,  0.3105,  0.2372, -0.1638,  0.2073,  0.0406,
           0.1716],
         [ 0.2840,  0.0231,  0.3085,  0.2343, -0.1631,  0.2042,  0.0382,
           0.1704]]], grad_fn=<ViewBackward0>)

== Multi-Head Attention Weights Shape ==
torch.Size([1, 2, 4, 4])
BERT Layer 0 Head 0 Attention Weights:
 tensor([[0.0919, 0.2086, 0.0697, 0.0806, 0.1075, 0.4416],
        [0.1786, 0.1911, 0.1457, 0.1230, 0.1925, 0.1691],
        [0.1342, 0.1160, 0.1534, 0.2414, 0.1181, 0.2369],
        [0.1073, 0.1511, 0.2138, 0.0881, 0.1583, 0.2812],
        [0.1148, 0.0844, 0.1062, 0.1242, 0.0834, 0.4870],
        [0.2008, 0.1855, 0.1117, 0.0766, 0.1336, 0.2917]],
       grad_fn=<SelectBackward0>)
